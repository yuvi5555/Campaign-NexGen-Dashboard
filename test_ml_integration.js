// Test script to verify ML integration
import fetch from 'node-fetch';

async function testMLIntegration() {
  console.log('ğŸ§ª Testing ML Integration...\n');

  // Test 1: Check if backend server is running
  try {
    console.log('1. Testing backend server connection...');
    const response = await fetch('http://localhost:3001/api/predict', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        metric: 'Clicks',
        company: 'TestCorp',
        campaign_type: 'Email',
        channel: 'Social',
        target_audience: 'General',
        location: 'Global',
        language: 'English',
        customer_segment: 'All'
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('âœ… Backend server is running');
      console.log('ğŸ“Š Prediction result:', {
        metric: data.metric,
        prediction: data.prediction?.toFixed(2),
        confidence: (data.confidence * 100)?.toFixed(1) + '%',
        models_used: data.models_used
      });
    } else {
      console.log('âŒ Backend server error:', response.status, response.statusText);
    }
  } catch (error) {
    console.log('âŒ Backend server not running:', error.message);
    console.log('ğŸ’¡ Please start the backend server with: cd backend && node server.js');
    return;
  }

  // Test 2: Test query endpoint
  try {
    console.log('\n2. Testing query endpoint...');
    const response = await fetch('http://localhost:3001/api/query', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        query: 'What is the average ROI by channel?'
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('âœ… Query endpoint is working');
      console.log('ğŸ“Š Query result type:', Array.isArray(data.result) ? 'Array' : typeof data.result);
    } else {
      console.log('âŒ Query endpoint error:', response.status, response.statusText);
    }
  } catch (error) {
    console.log('âŒ Query endpoint error:', error.message);
  }

  // Test 3: Test model performance endpoint
  try {
    console.log('\n3. Testing model performance endpoint...');
    const response = await fetch('http://localhost:3001/api/models/performance');

    if (response.ok) {
      const data = await response.json();
      console.log('âœ… Model performance endpoint is working');
      console.log('ğŸ“Š Available models:', Object.keys(data));
    } else {
      console.log('âŒ Model performance endpoint error:', response.status, response.statusText);
    }
  } catch (error) {
    console.log('âŒ Model performance endpoint error:', error.message);
  }

  // Test 4: Test feature importance endpoint
  try {
    console.log('\n4. Testing feature importance endpoint...');
    const response = await fetch('http://localhost:3001/api/models/features/clicks');

    if (response.ok) {
      const data = await response.json();
      console.log('âœ… Feature importance endpoint is working');
      console.log('ğŸ“Š Number of features:', data.length);
    } else {
      console.log('âŒ Feature importance endpoint error:', response.status, response.statusText);
    }
  } catch (error) {
    console.log('âŒ Feature importance endpoint error:', error.message);
  }

  console.log('\nğŸ‰ ML Integration test completed!');
  console.log('\nğŸ’¡ To start the frontend, run: npm run dev');
}

testMLIntegration().catch(console.error);
